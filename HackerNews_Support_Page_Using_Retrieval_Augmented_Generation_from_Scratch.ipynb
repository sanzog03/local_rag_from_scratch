{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbEJmXIlqdNr"
      },
      "source": [
        "# Data preparation/Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpHd5jG5qdNs",
        "outputId": "bad25379-6db7-4540-f7d8-35f8c9edc71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "legal.json          newsfaq.json        newsguidelines.json security.json\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIXWgjxxqdNs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import spacy\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_jekn5iqdNt"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path) as f:\n",
        "        data = json.load(f)\n",
        "        content = data[\"content\"]\n",
        "        url = data[\"url\"]\n",
        "        doc = nlp(content)\n",
        "\n",
        "        return [{\"text\": sent.text, \"url\": url} for sent in doc.sents]\n",
        "\n",
        "chunks = [\n",
        "    chunk\n",
        "    for file in os.listdir(\"data\")\n",
        "    for chunk in process_file(os.path.join(\"data\", file))\n",
        "]\n",
        "\n",
        "chunks = [{\"id\": i, **chunk} for i, chunk in enumerate(chunks)]\n",
        "\n",
        "with open(\"chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juLmch1vqdNt"
      },
      "source": [
        "# Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIyJV8q6qdNt"
      },
      "outputs": [],
      "source": [
        "sentences = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "embeddings = model.encode(sentences, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-G4xA4zqdNt"
      },
      "outputs": [],
      "source": [
        "faiss_index = faiss.IndexFlatIP(model.get_sentence_embedding_dimension())\n",
        "faiss_index.add(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ukxotUqdNt"
      },
      "source": [
        "# Retrieval/Prompt preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItqK_PNQqdNt"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer using the provided contexts. Every answer you generate should have citations in this pattern  \"Answer [position].\", for example: \"Earth is round [1][2].,\" if it's relevant.\n",
        "\n",
        "Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state, \"The provided contexts does not have the answer.\"\n",
        "\n",
        "User question: {}\n",
        "\n",
        "Contexts:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZoA6CBhqdNt"
      },
      "outputs": [],
      "source": [
        "k = 50\n",
        "question = \"Why HackerNews is so popular?\"\n",
        "\n",
        "query_embedding = model.encode([question])\n",
        "distances, indices = faiss_index.search(query_embedding, k)\n",
        "\n",
        "context = \"\\n\".join([f\"{i}. {sentences[index]}\" for i, index in enumerate(indices[0])])\n",
        "prompt = f\"{base_prompt.format(question, context)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAzTWesqdNt"
      },
      "source": [
        "# Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCDuV2nCqdNt"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-16k-0613\",\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
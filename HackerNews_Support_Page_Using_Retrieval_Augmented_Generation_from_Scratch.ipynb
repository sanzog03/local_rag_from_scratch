{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbEJmXIlqdNr"
      },
      "source": [
        "# Data preparation/Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CpHd5jG5qdNs",
        "outputId": "bad25379-6db7-4540-f7d8-35f8c9edc71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.json\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UIXWgjxxqdNs"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sentence_transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import spacy\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_jekn5iqdNt"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path) as f:\n",
        "        data = json.load(f)\n",
        "        content = data[\"content\"]\n",
        "        url = data[\"url\"]\n",
        "        doc = nlp(content)\n",
        "\n",
        "        return [{\"text\": sent.text, \"url\": url} for sent in doc.sents]\n",
        "\n",
        "chunks = [\n",
        "    chunk\n",
        "    for file in os.listdir(\"data\")\n",
        "    for chunk in process_file(os.path.join(\"data\", file))\n",
        "]\n",
        "\n",
        "chunks = [{\"id\": i, **chunk} for i, chunk in enumerate(chunks)]\n",
        "\n",
        "with open(\"chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juLmch1vqdNt"
      },
      "source": [
        "# Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIyJV8q6qdNt"
      },
      "outputs": [],
      "source": [
        "sentences = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "embeddings = model.encode(sentences, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-G4xA4zqdNt"
      },
      "outputs": [],
      "source": [
        "faiss_index = faiss.IndexFlatIP(model.get_sentence_embedding_dimension())\n",
        "faiss_index.add(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ukxotUqdNt"
      },
      "source": [
        "# Retrieval/Prompt preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItqK_PNQqdNt"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer using the provided contexts. Every answer you generate should have citations in this pattern  \"Answer [position].\", for example: \"Earth is round [1][2].,\" if it's relevant.\n",
        "\n",
        "Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state, \"The provided contexts does not have the answer.\"\n",
        "\n",
        "User question: {}\n",
        "\n",
        "Contexts:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZoA6CBhqdNt"
      },
      "outputs": [],
      "source": [
        "k = 50\n",
        "question = \"Why HackerNews is so popular?\"\n",
        "\n",
        "query_embedding = model.encode([question])\n",
        "distances, indices = faiss_index.search(query_embedding, k)\n",
        "\n",
        "context = \"\\n\".join([f\"{i}. {sentences[index]}\" for i, index in enumerate(indices[0])])\n",
        "prompt = f\"{base_prompt.format(question, context)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAzTWesqdNt"
      },
      "source": [
        "# Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCDuV2nCqdNt"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key = \"sk-deepseek-dummy-key\",    \n",
        ")\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-r1:8b\",\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful and informative assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n",
        "except openai.OpenAIError as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"\\nPlease ensure your Ollama server is running and the model is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check OPEN AI using Deepseek R1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, so I'm trying to understand what lazy evaluation is in programming. I've heard the term before, maybe in my studies or online articles, but I'm not entirely sure about its meaning. Let me try to break it down step by step.\n",
            "\n",
            "First, I know that evaluation in programming usually refers to how expressions are processed and values are determined. Lazy evaluation must be a specific way of handling this. Maybe it's related to when something is evaluated or computed only when needed instead of upfront. \n",
            "\n",
            "I remember hearing about lazy loading in databases, where certain data isn't fetched until it's actually accessed. Could that be similar? So perhaps in programming, lazy evaluation means that a value or computation is done only when it's required, not beforehand. That would save resources by avoiding unnecessary work unless it's necessary.\n",
            "\n",
            "Wait, so if I have a function that needs some data, and instead of loading the data right away, I wait until the function is called and then load it then? That makes sense for performance optimization. It ensures that we don't waste time or resources on something that might not be used.\n",
            "\n",
            "But how does this apply in different programming contexts? Maybe in functional programming languages like Haskell or Lisp, which are said to use lazy evaluation extensively. In those languages, functions are evaluated lazily, meaning they're only executed when their results are needed. That could prevent infinite loops or stack overflows because the program doesn't evaluate everything upfront.\n",
            "\n",
            "Let me think about an example. Suppose I have a list of numbers, and I want to process each number in a loop. In a language with eager evaluation, like JavaScript, all elements would be processed immediately when the loop starts. But in a language with lazy evaluation, maybe the loop waits until it's told to process each element as they're accessed.\n",
            "\n",
            "Another point is that in some databases or APIs, queries are executed on demand instead of being run every time. So if I have a web page that fetches data from an API, lazy evaluation would mean the data isn't fetched until the user requests it, which saves bandwidth and processing time.\n",
            "\n",
            "I should also consider how this affects data structures and algorithms. Lazy evaluation can lead to more efficient use of memory because objects are created only when needed. This is especially useful in languages with garbage collection since it reduces the amount of data that might be unused.\n",
            "\n",
            "But are there any downsides? Well, if too much is deferred, it could cause delays or unexpected waits when something is finally accessed. For example, in a game loop, if rendering is lazy, each frame might take longer to render once the data is fetched. So it's all about balance and knowing when to evaluate things lazily versus eagerly.\n",
            "\n",
            "I also wonder how this compares to memoization. Memoization caches results so that they don't need to be recomputed, which is a form of optimization but not exactly the same as lazy evaluation. Maybe they are related in some ways, especially in functional programming where both can help optimize performance by avoiding redundant work.\n",
            "\n",
            "In summary, lazy evaluation seems to be about deferring computations or data fetching until it's necessary. This approach can lead to better performance and efficiency, especially in resource-constrained environments. It's widely used in certain programming languages and patterns to optimize resource usage.\n",
            "</think>\n",
            "\n",
            "Lazy evaluation in programming is a strategy where the evaluation of expressions or computations is deferred until they are actually needed. This approach optimizes resource use by ensuring that computations or data fetching occur only when necessary, rather than upfront. Here's a structured summary of the concept:\n",
            "\n",
            "1. **Definition**: Lazy evaluation means that values or computations are processed only when they are required, not beforehand. This can save resources and avoid unnecessary work.\n",
            "\n",
            "2. **Contexts of Use**:\n",
            "   - **Functional Programming**: Languages like Haskell and Lisp use lazy evaluation, where functions are executed only when their results are needed, preventing issues like infinite loops.\n",
            "   - **Database Management**: Similar to lazy loading in databases, data is fetched or processed only when accessed, conserving bandwidth and processing time.\n",
            "   - **Web Development**: APIs and data fetching can be optimized by deferring requests until they're needed.\n",
            "\n",
            "3. **Examples**:\n",
            "   - In a loop processing a list, elements are accessed only as needed, reducing upfront computation.\n",
            "   - Web pages fetch data from APIs on demand, saving resources.\n",
            "\n",
            "4. **Benefits**:\n",
            "   - **Efficiency**: Reduces resource consumption and avoids unnecessary computations.\n",
            "   - **Memory Management**: Lower memory usage as objects are created only when needed, beneficial in languages with garbage collection.\n",
            "   \n",
            "5. **Considerations**:\n",
            "   - Balancing deferred evaluation to avoid delays or performance issues, especially in real-time applications like gaming.\n",
            "\n",
            "6. **Comparison with Memoization**: While related in optimizing performance, memoization caches results to avoid recomputation, distinct from lazy evaluation's focus on deferring processing.\n",
            "\n",
            "In essence, lazy evaluation enhances efficiency by deferring computations and data fetching until they are needed, making it particularly useful in resource-sensitive environments.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# --- New syntax for openai library v1.0.0+ ---\n",
        "# Import the OpenAI client`` class\n",
        "from openai import OpenAI\n",
        "\n",
        "# Create an instance of the OpenAI client.\n",
        "# This client object will handle the API calls.\n",
        "# Set the base_url to the correct HTTP endpoint for Ollama.\n",
        "# Note that it's 'http' not 'https'\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key = \"sk-deepseek-dummy-key\",    \n",
        ")\n",
        "\n",
        "# Define a sample prompt for the model.\n",
        "prompt = \"Explain the concept of 'lazy evaluation' in programming.\"\n",
        "\n",
        "try:\n",
        "    # Use the client instance to make the API call with the new syntax.\n",
        "    # The method is now `client.chat.completions.create`.\n",
        "    # The arguments are largely the same.\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-r1:8b\",\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful and informative assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Access the content from the response object. The syntax for this also changed\n",
        "    # from a dictionary-like access to a more object-oriented one.\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "except openai.OpenAIError as e:\n",
        "    # The exception class has changed, but you can still catch it.\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"\\nPlease ensure your Ollama server is running and the model is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbEJmXIlqdNr"
      },
      "source": [
        "# Data preparation/Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CpHd5jG5qdNs",
        "outputId": "bad25379-6db7-4540-f7d8-35f8c9edc71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test.json\n"
          ]
        }
      ],
      "source": [
        "!ls data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIXWgjxxqdNs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sthapa/Devs/rag/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'faiss'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import spacy\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "openai.api_base = \"https://localhost:11434/v1\"\n",
        "openai.api_key = \"sk-deepseek-dummy-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_jekn5iqdNt"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def process_file(file_path):\n",
        "    with open(file_path) as f:\n",
        "        data = json.load(f)\n",
        "        content = data[\"content\"]\n",
        "        url = data[\"url\"]\n",
        "        doc = nlp(content)\n",
        "\n",
        "        return [{\"text\": sent.text, \"url\": url} for sent in doc.sents]\n",
        "\n",
        "chunks = [\n",
        "    chunk\n",
        "    for file in os.listdir(\"data\")\n",
        "    for chunk in process_file(os.path.join(\"data\", file))\n",
        "]\n",
        "\n",
        "chunks = [{\"id\": i, **chunk} for i, chunk in enumerate(chunks)]\n",
        "\n",
        "with open(\"chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juLmch1vqdNt"
      },
      "source": [
        "# Vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIyJV8q6qdNt"
      },
      "outputs": [],
      "source": [
        "sentences = [chunk[\"text\"] for chunk in chunks]\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "embeddings = model.encode(sentences, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-G4xA4zqdNt"
      },
      "outputs": [],
      "source": [
        "faiss_index = faiss.IndexFlatIP(model.get_sentence_embedding_dimension())\n",
        "faiss_index.add(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7ukxotUqdNt"
      },
      "source": [
        "# Retrieval/Prompt preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItqK_PNQqdNt"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"\"\"You are an AI assistant. Your task is to understand the user question, and provide an answer using the provided contexts. Every answer you generate should have citations in this pattern  \"Answer [position].\", for example: \"Earth is round [1][2].,\" if it's relevant.\n",
        "\n",
        "Your answers are correct, high-quality, and written by an domain expert. If the provided context does not contain the answer, simply state, \"The provided contexts does not have the answer.\"\n",
        "\n",
        "User question: {}\n",
        "\n",
        "Contexts:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZoA6CBhqdNt"
      },
      "outputs": [],
      "source": [
        "k = 50\n",
        "question = \"Why HackerNews is so popular?\"\n",
        "\n",
        "query_embedding = model.encode([question])\n",
        "distances, indices = faiss_index.search(query_embedding, k)\n",
        "\n",
        "context = \"\\n\".join([f\"{i}. {sentences[index]}\" for i, index in enumerate(indices[0])])\n",
        "prompt = f\"{base_prompt.format(question, context)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAzTWesqdNt"
      },
      "source": [
        "# Answer Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCDuV2nCqdNt"
      },
      "outputs": [],
      "source": [
        "response = openai.ChatCompletion.create(\n",
        "    model=\"deepseek-coder:8b\",\n",
        "    temperature=0,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt},\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check OPEN AI using Deepseek R1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://localhost:11434/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msk-deepseek-dummy-key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeepseek-coder:8b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello world!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
            "File \u001b[0;32m~/Devs/rag/.venv/lib/python3.9/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# --- New syntax for openai library v1.0.0+ ---\n",
        "# Import the OpenAI client class\n",
        "from openai import OpenAI\n",
        "\n",
        "# Create an instance of the OpenAI client.\n",
        "# This client object will handle the API calls.\n",
        "# Set the base_url to the correct HTTP endpoint for Ollama.\n",
        "# Note that it's 'http' not 'https'\n",
        "client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key = \"sk-deepseek-dummy-key\",    \n",
        ")\n",
        "\n",
        "# Define a sample prompt for the model.\n",
        "prompt = \"Explain the concept of 'lazy evaluation' in programming.\"\n",
        "\n",
        "try:\n",
        "    # Use the client instance to make the API call with the new syntax.\n",
        "    # The method is now `client.chat.completions.create`.\n",
        "    # The arguments are largely the same.\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-r1:8b\",\n",
        "        temperature=0.7,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful and informative assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Access the content from the response object. The syntax for this also changed\n",
        "    # from a dictionary-like access to a more object-oriented one.\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "except openai.OpenAIError as e:\n",
        "    # The exception class has changed, but you can still catch it.\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"\\nPlease ensure your Ollama server is running and the model is downloaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
